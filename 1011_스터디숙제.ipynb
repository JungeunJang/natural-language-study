{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e922dd073470bdcc017ae3abd31d6491d6ed7bf31c1d559806e5511bfea88b81"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Embedding(20000, 128, input_length = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16\n"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index)+1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
    }
   ],
   "source": [
    "X_encoded = t.texts_to_sequences(sentences)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4\n"
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 1  2  3  4]\n [ 5  6  0  0]\n [ 7  8  0  0]\n [ 9 10  0  0]\n [11 12  0  0]\n [13  0  0  0]\n [14 15  0  0]]\n"
    }
   ],
   "source": [
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length = max_len))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n1/1 - 0s - loss: 0.6991 - acc: 0.5714\nEpoch 2/100\n1/1 - 0s - loss: 0.6971 - acc: 0.5714\nEpoch 3/100\n1/1 - 0s - loss: 0.6951 - acc: 0.5714\nEpoch 4/100\n1/1 - 0s - loss: 0.6931 - acc: 0.5714\nEpoch 5/100\n1/1 - 0s - loss: 0.6912 - acc: 0.5714\nEpoch 6/100\n1/1 - 0s - loss: 0.6892 - acc: 0.5714\nEpoch 7/100\n1/1 - 0s - loss: 0.6872 - acc: 0.5714\nEpoch 8/100\n1/1 - 0s - loss: 0.6852 - acc: 0.5714\nEpoch 9/100\n1/1 - 0s - loss: 0.6832 - acc: 0.5714\nEpoch 10/100\n1/1 - 0s - loss: 0.6813 - acc: 0.7143\nEpoch 11/100\n1/1 - 0s - loss: 0.6793 - acc: 0.7143\nEpoch 12/100\n1/1 - 0s - loss: 0.6773 - acc: 0.7143\nEpoch 13/100\n1/1 - 0s - loss: 0.6754 - acc: 0.7143\nEpoch 14/100\n1/1 - 0s - loss: 0.6734 - acc: 0.7143\nEpoch 15/100\n1/1 - 0s - loss: 0.6715 - acc: 0.7143\nEpoch 16/100\n1/1 - 0s - loss: 0.6695 - acc: 0.7143\nEpoch 17/100\n1/1 - 0s - loss: 0.6676 - acc: 0.8571\nEpoch 18/100\n1/1 - 0s - loss: 0.6656 - acc: 0.8571\nEpoch 19/100\n1/1 - 0s - loss: 0.6636 - acc: 0.8571\nEpoch 20/100\n1/1 - 0s - loss: 0.6617 - acc: 0.8571\nEpoch 21/100\n1/1 - 0s - loss: 0.6597 - acc: 0.8571\nEpoch 22/100\n1/1 - 0s - loss: 0.6578 - acc: 0.8571\nEpoch 23/100\n1/1 - 0s - loss: 0.6558 - acc: 0.8571\nEpoch 24/100\n1/1 - 0s - loss: 0.6538 - acc: 0.8571\nEpoch 25/100\n1/1 - 0s - loss: 0.6519 - acc: 0.8571\nEpoch 26/100\n1/1 - 0s - loss: 0.6499 - acc: 0.8571\nEpoch 27/100\n1/1 - 0s - loss: 0.6479 - acc: 0.8571\nEpoch 28/100\n1/1 - 0s - loss: 0.6460 - acc: 0.8571\nEpoch 29/100\n1/1 - 0s - loss: 0.6440 - acc: 0.8571\nEpoch 30/100\n1/1 - 0s - loss: 0.6420 - acc: 0.8571\nEpoch 31/100\n1/1 - 0s - loss: 0.6400 - acc: 0.8571\nEpoch 32/100\n1/1 - 0s - loss: 0.6380 - acc: 0.8571\nEpoch 33/100\n1/1 - 0s - loss: 0.6360 - acc: 0.8571\nEpoch 34/100\n1/1 - 0s - loss: 0.6340 - acc: 0.8571\nEpoch 35/100\n1/1 - 0s - loss: 0.6320 - acc: 0.8571\nEpoch 36/100\n1/1 - 0s - loss: 0.6300 - acc: 0.8571\nEpoch 37/100\n1/1 - 0s - loss: 0.6280 - acc: 1.0000\nEpoch 38/100\n1/1 - 0s - loss: 0.6260 - acc: 1.0000\nEpoch 39/100\n1/1 - 0s - loss: 0.6240 - acc: 1.0000\nEpoch 40/100\n1/1 - 0s - loss: 0.6219 - acc: 1.0000\nEpoch 41/100\n1/1 - 0s - loss: 0.6199 - acc: 1.0000\nEpoch 42/100\n1/1 - 0s - loss: 0.6179 - acc: 1.0000\nEpoch 43/100\n1/1 - 0s - loss: 0.6158 - acc: 1.0000\nEpoch 44/100\n1/1 - 0s - loss: 0.6138 - acc: 1.0000\nEpoch 45/100\n1/1 - 0s - loss: 0.6117 - acc: 1.0000\nEpoch 46/100\n1/1 - 0s - loss: 0.6096 - acc: 1.0000\nEpoch 47/100\n1/1 - 0s - loss: 0.6076 - acc: 1.0000\nEpoch 48/100\n1/1 - 0s - loss: 0.6055 - acc: 1.0000\nEpoch 49/100\n1/1 - 0s - loss: 0.6034 - acc: 1.0000\nEpoch 50/100\n1/1 - 0s - loss: 0.6013 - acc: 1.0000\nEpoch 51/100\n1/1 - 0s - loss: 0.5992 - acc: 1.0000\nEpoch 52/100\n1/1 - 0s - loss: 0.5971 - acc: 1.0000\nEpoch 53/100\n1/1 - 0s - loss: 0.5950 - acc: 1.0000\nEpoch 54/100\n1/1 - 0s - loss: 0.5929 - acc: 1.0000\nEpoch 55/100\n1/1 - 0s - loss: 0.5908 - acc: 1.0000\nEpoch 56/100\n1/1 - 0s - loss: 0.5886 - acc: 1.0000\nEpoch 57/100\n1/1 - 0s - loss: 0.5865 - acc: 1.0000\nEpoch 58/100\n1/1 - 0s - loss: 0.5844 - acc: 1.0000\nEpoch 59/100\n1/1 - 0s - loss: 0.5822 - acc: 1.0000\nEpoch 60/100\n1/1 - 0s - loss: 0.5801 - acc: 1.0000\nEpoch 61/100\n1/1 - 0s - loss: 0.5779 - acc: 1.0000\nEpoch 62/100\n1/1 - 0s - loss: 0.5758 - acc: 1.0000\nEpoch 63/100\n1/1 - 0s - loss: 0.5736 - acc: 1.0000\nEpoch 64/100\n1/1 - 0s - loss: 0.5714 - acc: 1.0000\nEpoch 65/100\n1/1 - 0s - loss: 0.5693 - acc: 1.0000\nEpoch 66/100\n1/1 - 0s - loss: 0.5671 - acc: 1.0000\nEpoch 67/100\n1/1 - 0s - loss: 0.5649 - acc: 1.0000\nEpoch 68/100\n1/1 - 0s - loss: 0.5627 - acc: 1.0000\nEpoch 69/100\n1/1 - 0s - loss: 0.5605 - acc: 1.0000\nEpoch 70/100\n1/1 - 0s - loss: 0.5583 - acc: 1.0000\nEpoch 71/100\n1/1 - 0s - loss: 0.5561 - acc: 1.0000\nEpoch 72/100\n1/1 - 0s - loss: 0.5539 - acc: 1.0000\nEpoch 73/100\n1/1 - 0s - loss: 0.5517 - acc: 1.0000\nEpoch 74/100\n1/1 - 0s - loss: 0.5495 - acc: 1.0000\nEpoch 75/100\n1/1 - 0s - loss: 0.5473 - acc: 1.0000\nEpoch 76/100\n1/1 - 0s - loss: 0.5451 - acc: 1.0000\nEpoch 77/100\n1/1 - 0s - loss: 0.5429 - acc: 1.0000\nEpoch 78/100\n1/1 - 0s - loss: 0.5406 - acc: 1.0000\nEpoch 79/100\n1/1 - 0s - loss: 0.5384 - acc: 1.0000\nEpoch 80/100\n1/1 - 0s - loss: 0.5362 - acc: 1.0000\nEpoch 81/100\n1/1 - 0s - loss: 0.5339 - acc: 1.0000\nEpoch 82/100\n1/1 - 0s - loss: 0.5317 - acc: 1.0000\nEpoch 83/100\n1/1 - 0s - loss: 0.5295 - acc: 1.0000\nEpoch 84/100\n1/1 - 0s - loss: 0.5272 - acc: 1.0000\nEpoch 85/100\n1/1 - 0s - loss: 0.5250 - acc: 1.0000\nEpoch 86/100\n1/1 - 0s - loss: 0.5227 - acc: 1.0000\nEpoch 87/100\n1/1 - 0s - loss: 0.5205 - acc: 1.0000\nEpoch 88/100\n1/1 - 0s - loss: 0.5182 - acc: 1.0000\nEpoch 89/100\n1/1 - 0s - loss: 0.5160 - acc: 1.0000\nEpoch 90/100\n1/1 - 0s - loss: 0.5137 - acc: 1.0000\nEpoch 91/100\n1/1 - 0s - loss: 0.5115 - acc: 1.0000\nEpoch 92/100\n1/1 - 0s - loss: 0.5092 - acc: 1.0000\nEpoch 93/100\n1/1 - 0s - loss: 0.5069 - acc: 1.0000\nEpoch 94/100\n1/1 - 0s - loss: 0.5047 - acc: 1.0000\nEpoch 95/100\n1/1 - 0s - loss: 0.5024 - acc: 1.0000\nEpoch 96/100\n1/1 - 0s - loss: 0.5002 - acc: 1.0000\nEpoch 97/100\n1/1 - 0s - loss: 0.4979 - acc: 1.0000\nEpoch 98/100\n1/1 - 0s - loss: 0.4956 - acc: 1.0000\nEpoch 99/100\n1/1 - 0s - loss: 0.4934 - acc: 1.0000\nEpoch 100/100\n1/1 - 0s - loss: 0.4911 - acc: 1.0000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.keras.callbacks.History at 0x2789d4146c8&gt;"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.fit(X_train, y_train, epochs =100, verbose=2)"
   ]
  },
  {
   "source": [
    "# 사전 훈련된 워드 임베딩 사용하기 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 1  2  3  4]\n [ 5  6  0  0]\n [ 7  8  0  0]\n [ 9 10  0  0]\n [11 12  0  0]\n [13  0  0  0]\n [14 15  0  0]]\n[1 0 0 1 1 0 1]\n"
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "source": [
    "# 사전 훈련된 GloVe 사용하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[&#39;the&#39;, &#39;-0.038194&#39;, &#39;-0.24487&#39;, &#39;0.72812&#39;, &#39;-0.39961&#39;, &#39;0.083172&#39;, &#39;0.043953&#39;, &#39;-0.39141&#39;, &#39;0.3344&#39;, &#39;-0.57545&#39;, &#39;0.087459&#39;, &#39;0.28787&#39;, &#39;-0.06731&#39;, &#39;0.30906&#39;, &#39;-0.26384&#39;, &#39;-0.13231&#39;, &#39;-0.20757&#39;, &#39;0.33395&#39;, &#39;-0.33848&#39;, &#39;-0.31743&#39;, &#39;-0.48336&#39;, &#39;0.1464&#39;, &#39;-0.37304&#39;, &#39;0.34577&#39;, &#39;0.052041&#39;, &#39;0.44946&#39;, &#39;-0.46971&#39;, &#39;0.02628&#39;, &#39;-0.54155&#39;, &#39;-0.15518&#39;, &#39;-0.14107&#39;, &#39;-0.039722&#39;, &#39;0.28277&#39;, &#39;0.14393&#39;, &#39;0.23464&#39;, &#39;-0.31021&#39;, &#39;0.086173&#39;, &#39;0.20397&#39;, &#39;0.52624&#39;, &#39;0.17164&#39;, &#39;-0.082378&#39;, &#39;-0.71787&#39;, &#39;-0.41531&#39;, &#39;0.20335&#39;, &#39;-0.12763&#39;, &#39;0.41367&#39;, &#39;0.55187&#39;, &#39;0.57908&#39;, &#39;-0.33477&#39;, &#39;-0.36559&#39;, &#39;-0.54857&#39;, &#39;-0.062892&#39;, &#39;0.26584&#39;, &#39;0.30205&#39;, &#39;0.99775&#39;, &#39;-0.80481&#39;, &#39;-3.0243&#39;, &#39;0.01254&#39;, &#39;-0.36942&#39;, &#39;2.2167&#39;, &#39;0.72201&#39;, &#39;-0.24978&#39;, &#39;0.92136&#39;, &#39;0.034514&#39;, &#39;0.46745&#39;, &#39;1.1079&#39;, &#39;-0.19358&#39;, &#39;-0.074575&#39;, &#39;0.23353&#39;, &#39;-0.052062&#39;, &#39;-0.22044&#39;, &#39;0.057162&#39;, &#39;-0.15806&#39;, &#39;-0.30798&#39;, &#39;-0.41625&#39;, &#39;0.37972&#39;, &#39;0.15006&#39;, &#39;-0.53212&#39;, &#39;-0.2055&#39;, &#39;-1.2526&#39;, &#39;0.071624&#39;, &#39;0.70565&#39;, &#39;0.49744&#39;, &#39;-0.42063&#39;, &#39;0.26148&#39;, &#39;-1.538&#39;, &#39;-0.30223&#39;, &#39;-0.073438&#39;, &#39;-0.28312&#39;, &#39;0.37104&#39;, &#39;-0.25217&#39;, &#39;0.016215&#39;, &#39;-0.017099&#39;, &#39;-0.38984&#39;, &#39;0.87424&#39;, &#39;-0.72569&#39;, &#39;-0.51058&#39;, &#39;-0.52028&#39;, &#39;-0.1459&#39;, &#39;0.8278&#39;, &#39;0.27062&#39;]\nthe\n[&#39;,&#39;, &#39;-0.10767&#39;, &#39;0.11053&#39;, &#39;0.59812&#39;, &#39;-0.54361&#39;, &#39;0.67396&#39;, &#39;0.10663&#39;, &#39;0.038867&#39;, &#39;0.35481&#39;, &#39;0.06351&#39;, &#39;-0.094189&#39;, &#39;0.15786&#39;, &#39;-0.81665&#39;, &#39;0.14172&#39;, &#39;0.21939&#39;, &#39;0.58505&#39;, &#39;-0.52158&#39;, &#39;0.22783&#39;, &#39;-0.16642&#39;, &#39;-0.68228&#39;, &#39;0.3587&#39;, &#39;0.42568&#39;, &#39;0.19021&#39;, &#39;0.91963&#39;, &#39;0.57555&#39;, &#39;0.46185&#39;, &#39;0.42363&#39;, &#39;-0.095399&#39;, &#39;-0.42749&#39;, &#39;-0.16567&#39;, &#39;-0.056842&#39;, &#39;-0.29595&#39;, &#39;0.26037&#39;, &#39;-0.26606&#39;, &#39;-0.070404&#39;, &#39;-0.27662&#39;, &#39;0.15821&#39;, &#39;0.69825&#39;, &#39;0.43081&#39;, &#39;0.27952&#39;, &#39;-0.45437&#39;, &#39;-0.33801&#39;, &#39;-0.58184&#39;, &#39;0.22364&#39;, &#39;-0.5778&#39;, &#39;-0.26862&#39;, &#39;-0.20425&#39;, &#39;0.56394&#39;, &#39;-0.58524&#39;, &#39;-0.14365&#39;, &#39;-0.64218&#39;, &#39;0.0054697&#39;, &#39;-0.35248&#39;, &#39;0.16162&#39;, &#39;1.1796&#39;, &#39;-0.47674&#39;, &#39;-2.7553&#39;, &#39;-0.1321&#39;, &#39;-0.047729&#39;, &#39;1.0655&#39;, &#39;1.1034&#39;, &#39;-0.2208&#39;, &#39;0.18669&#39;, &#39;0.13177&#39;, &#39;0.15117&#39;, &#39;0.7131&#39;, &#39;-0.35215&#39;, &#39;0.91348&#39;, &#39;0.61783&#39;, &#39;0.70992&#39;, &#39;0.23955&#39;, &#39;-0.14571&#39;, &#39;-0.37859&#39;, &#39;-0.045959&#39;, &#39;-0.47368&#39;, &#39;0.2385&#39;, &#39;0.20536&#39;, &#39;-0.18996&#39;, &#39;0.32507&#39;, &#39;-1.1112&#39;, &#39;-0.36341&#39;, &#39;0.98679&#39;, &#39;-0.084776&#39;, &#39;-0.54008&#39;, &#39;0.11726&#39;, &#39;-1.0194&#39;, &#39;-0.24424&#39;, &#39;0.12771&#39;, &#39;0.013884&#39;, &#39;0.080374&#39;, &#39;-0.35414&#39;, &#39;0.34951&#39;, &#39;-0.7226&#39;, &#39;0.37549&#39;, &#39;0.4441&#39;, &#39;-0.99059&#39;, &#39;0.61214&#39;, &#39;-0.35111&#39;, &#39;-0.83155&#39;, &#39;0.45293&#39;, &#39;0.082577&#39;]\n,\n"
    }
   ],
   "source": [
    "n = 0\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    print(word_vector)\n",
    "    word = word_vector[0]\n",
    "    print(word)\n",
    "    n = n+1\n",
    "    if n==2:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "&lt;class &#39;list&#39;&gt;\n101\n"
    }
   ],
   "source": [
    "print(type(word_vector))\n",
    "print(len(word_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "400000개의 Embedding vector가 있습니다.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "embedding_dict = dict()\n",
    "\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n -0.21616   -0.19187   -0.032502   0.38025  ]\n100\n"
    }
   ],
   "source": [
    "print(embedding_dict['respectable'])\n",
    "print(len(embedding_dict['respectable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16, 100)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "dict_items([(&#39;nice&#39;, 1), (&#39;great&#39;, 2), (&#39;best&#39;, 3), (&#39;amazing&#39;, 4), (&#39;stop&#39;, 5), (&#39;lies&#39;, 6), (&#39;pitiful&#39;, 7), (&#39;nerd&#39;, 8), (&#39;excellent&#39;, 9), (&#39;work&#39;, 10), (&#39;supreme&#39;, 11), (&#39;quality&#39;, 12), (&#39;bad&#39;, 13), (&#39;highly&#39;, 14), (&#39;respectable&#39;, 15)])\n"
    }
   ],
   "source": [
    "print(t.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items():\n",
    "    temp = embedding_dict.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_dict[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 100, weights = [embedding_matrix], input_length=max_len, trainable = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n1/1 - 0s - loss: 0.6931 - acc: 0.4286\nEpoch 2/100\n1/1 - 0s - loss: 0.6931 - acc: 0.5714\nEpoch 3/100\n1/1 - 0s - loss: 0.6930 - acc: 0.5714\nEpoch 4/100\n1/1 - 0s - loss: 0.6929 - acc: 0.5714\nEpoch 5/100\n1/1 - 0s - loss: 0.6929 - acc: 0.5714\nEpoch 6/100\n1/1 - 0s - loss: 0.6928 - acc: 0.5714\nEpoch 7/100\n1/1 - 0s - loss: 0.6927 - acc: 0.5714\nEpoch 8/100\n1/1 - 0s - loss: 0.6927 - acc: 0.5714\nEpoch 9/100\n1/1 - 0s - loss: 0.6926 - acc: 0.5714\nEpoch 10/100\n1/1 - 0s - loss: 0.6925 - acc: 0.5714\nEpoch 11/100\n1/1 - 0s - loss: 0.6924 - acc: 0.5714\nEpoch 12/100\n1/1 - 0s - loss: 0.6924 - acc: 0.5714\nEpoch 13/100\n1/1 - 0s - loss: 0.6923 - acc: 0.5714\nEpoch 14/100\n1/1 - 0s - loss: 0.6922 - acc: 0.5714\nEpoch 15/100\n1/1 - 0s - loss: 0.6922 - acc: 0.5714\nEpoch 16/100\n1/1 - 0s - loss: 0.6921 - acc: 0.5714\nEpoch 17/100\n1/1 - 0s - loss: 0.6920 - acc: 0.5714\nEpoch 18/100\n1/1 - 0s - loss: 0.6920 - acc: 0.5714\nEpoch 19/100\n1/1 - 0s - loss: 0.6919 - acc: 0.5714\nEpoch 20/100\n1/1 - 0s - loss: 0.6918 - acc: 0.5714\nEpoch 21/100\n1/1 - 0s - loss: 0.6918 - acc: 0.5714\nEpoch 22/100\n1/1 - 0s - loss: 0.6917 - acc: 0.5714\nEpoch 23/100\n1/1 - 0s - loss: 0.6916 - acc: 0.5714\nEpoch 24/100\n1/1 - 0s - loss: 0.6916 - acc: 0.5714\nEpoch 25/100\n1/1 - 0s - loss: 0.6915 - acc: 0.5714\nEpoch 26/100\n1/1 - 0s - loss: 0.6915 - acc: 0.5714\nEpoch 27/100\n1/1 - 0s - loss: 0.6914 - acc: 0.5714\nEpoch 28/100\n1/1 - 0s - loss: 0.6913 - acc: 0.5714\nEpoch 29/100\n1/1 - 0s - loss: 0.6913 - acc: 0.5714\nEpoch 30/100\n1/1 - 0s - loss: 0.6912 - acc: 0.5714\nEpoch 31/100\n1/1 - 0s - loss: 0.6911 - acc: 0.5714\nEpoch 32/100\n1/1 - 0s - loss: 0.6911 - acc: 0.5714\nEpoch 33/100\n1/1 - 0s - loss: 0.6910 - acc: 0.5714\nEpoch 34/100\n1/1 - 0s - loss: 0.6909 - acc: 0.5714\nEpoch 35/100\n1/1 - 0s - loss: 0.6909 - acc: 0.5714\nEpoch 36/100\n1/1 - 0s - loss: 0.6908 - acc: 0.5714\nEpoch 37/100\n1/1 - 0s - loss: 0.6908 - acc: 0.5714\nEpoch 38/100\n1/1 - 0s - loss: 0.6907 - acc: 0.5714\nEpoch 39/100\n1/1 - 0s - loss: 0.6906 - acc: 0.5714\nEpoch 40/100\n1/1 - 0s - loss: 0.6906 - acc: 0.5714\nEpoch 41/100\n1/1 - 0s - loss: 0.6905 - acc: 0.5714\nEpoch 42/100\n1/1 - 0s - loss: 0.6905 - acc: 0.5714\nEpoch 43/100\n1/1 - 0s - loss: 0.6904 - acc: 0.5714\nEpoch 44/100\n1/1 - 0s - loss: 0.6904 - acc: 0.5714\nEpoch 45/100\n1/1 - 0s - loss: 0.6903 - acc: 0.5714\nEpoch 46/100\n1/1 - 0s - loss: 0.6902 - acc: 0.5714\nEpoch 47/100\n1/1 - 0s - loss: 0.6902 - acc: 0.5714\nEpoch 48/100\n1/1 - 0s - loss: 0.6901 - acc: 0.5714\nEpoch 49/100\n1/1 - 0s - loss: 0.6901 - acc: 0.5714\nEpoch 50/100\n1/1 - 0s - loss: 0.6900 - acc: 0.5714\nEpoch 51/100\n1/1 - 0s - loss: 0.6900 - acc: 0.5714\nEpoch 52/100\n1/1 - 0s - loss: 0.6899 - acc: 0.5714\nEpoch 53/100\n1/1 - 0s - loss: 0.6898 - acc: 0.5714\nEpoch 54/100\n1/1 - 0s - loss: 0.6898 - acc: 0.5714\nEpoch 55/100\n1/1 - 0s - loss: 0.6897 - acc: 0.5714\nEpoch 56/100\n1/1 - 0s - loss: 0.6897 - acc: 0.5714\nEpoch 57/100\n1/1 - 0s - loss: 0.6896 - acc: 0.5714\nEpoch 58/100\n1/1 - 0s - loss: 0.6896 - acc: 0.5714\nEpoch 59/100\n1/1 - 0s - loss: 0.6895 - acc: 0.5714\nEpoch 60/100\n1/1 - 0s - loss: 0.6895 - acc: 0.5714\nEpoch 61/100\n1/1 - 0s - loss: 0.6894 - acc: 0.5714\nEpoch 62/100\n1/1 - 0s - loss: 0.6894 - acc: 0.5714\nEpoch 63/100\n1/1 - 0s - loss: 0.6893 - acc: 0.5714\nEpoch 64/100\n1/1 - 0s - loss: 0.6893 - acc: 0.5714\nEpoch 65/100\n1/1 - 0s - loss: 0.6892 - acc: 0.5714\nEpoch 66/100\n1/1 - 0s - loss: 0.6892 - acc: 0.5714\nEpoch 67/100\n1/1 - 0s - loss: 0.6891 - acc: 0.5714\nEpoch 68/100\n1/1 - 0s - loss: 0.6891 - acc: 0.5714\nEpoch 69/100\n1/1 - 0s - loss: 0.6890 - acc: 0.5714\nEpoch 70/100\n1/1 - 0s - loss: 0.6890 - acc: 0.5714\nEpoch 71/100\n1/1 - 0s - loss: 0.6889 - acc: 0.5714\nEpoch 72/100\n1/1 - 0s - loss: 0.6889 - acc: 0.5714\nEpoch 73/100\n1/1 - 0s - loss: 0.6888 - acc: 0.5714\nEpoch 74/100\n1/1 - 0s - loss: 0.6888 - acc: 0.5714\nEpoch 75/100\n1/1 - 0s - loss: 0.6887 - acc: 0.5714\nEpoch 76/100\n1/1 - 0s - loss: 0.6887 - acc: 0.5714\nEpoch 77/100\n1/1 - 0s - loss: 0.6886 - acc: 0.5714\nEpoch 78/100\n1/1 - 0s - loss: 0.6886 - acc: 0.5714\nEpoch 79/100\n1/1 - 0s - loss: 0.6885 - acc: 0.5714\nEpoch 80/100\n1/1 - 0s - loss: 0.6885 - acc: 0.5714\nEpoch 81/100\n1/1 - 0s - loss: 0.6884 - acc: 0.5714\nEpoch 82/100\n1/1 - 0s - loss: 0.6884 - acc: 0.5714\nEpoch 83/100\n1/1 - 0s - loss: 0.6883 - acc: 0.5714\nEpoch 84/100\n1/1 - 0s - loss: 0.6883 - acc: 0.5714\nEpoch 85/100\n1/1 - 0s - loss: 0.6882 - acc: 0.5714\nEpoch 86/100\n1/1 - 0s - loss: 0.6882 - acc: 0.5714\nEpoch 87/100\n1/1 - 0s - loss: 0.6882 - acc: 0.5714\nEpoch 88/100\n1/1 - 0s - loss: 0.6881 - acc: 0.5714\nEpoch 89/100\n1/1 - 0s - loss: 0.6881 - acc: 0.5714\nEpoch 90/100\n1/1 - 0s - loss: 0.6880 - acc: 0.5714\nEpoch 91/100\n1/1 - 0s - loss: 0.6880 - acc: 0.5714\nEpoch 92/100\n1/1 - 0s - loss: 0.6879 - acc: 0.5714\nEpoch 93/100\n1/1 - 0s - loss: 0.6879 - acc: 0.5714\nEpoch 94/100\n1/1 - 0s - loss: 0.6879 - acc: 0.5714\nEpoch 95/100\n1/1 - 0s - loss: 0.6878 - acc: 0.5714\nEpoch 96/100\n1/1 - 0s - loss: 0.6878 - acc: 0.5714\nEpoch 97/100\n1/1 - 0s - loss: 0.6877 - acc: 0.5714\nEpoch 98/100\n1/1 - 0s - loss: 0.6877 - acc: 0.5714\nEpoch 99/100\n1/1 - 0s - loss: 0.6876 - acc: 0.5714\nEpoch 100/100\n1/1 - 0s - loss: 0.6876 - acc: 0.5714\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.keras.callbacks.History at 0x278b13d22c8&gt;"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "source": [
    "# 사전 훈련된 Word2Vec 사용하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3000000, 300)\n"
    }
   ],
   "source": [
    "print(word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(16, 300)"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items():\n",
    "    temp = get_vector(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
    }
   ],
   "source": [
    "print(word2vec_model['nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "단어 nice의 정수 인덱스 :  1\n"
    }
   ],
   "source": [
    "print('단어 nice의 정수 인덱스 : ', t.word_index['nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
    }
   ],
   "source": [
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/100\n1/1 - 0s - loss: 0.7456 - acc: 0.2857\nEpoch 2/100\n1/1 - 0s - loss: 0.7252 - acc: 0.5714\nEpoch 3/100\n1/1 - 0s - loss: 0.7053 - acc: 0.5714\nEpoch 4/100\n1/1 - 0s - loss: 0.6860 - acc: 0.5714\nEpoch 5/100\n1/1 - 0s - loss: 0.6672 - acc: 0.5714\nEpoch 6/100\n1/1 - 0s - loss: 0.6490 - acc: 0.7143\nEpoch 7/100\n1/1 - 0s - loss: 0.6314 - acc: 0.8571\nEpoch 8/100\n1/1 - 0s - loss: 0.6144 - acc: 0.8571\nEpoch 9/100\n1/1 - 0s - loss: 0.5979 - acc: 0.8571\nEpoch 10/100\n1/1 - 0s - loss: 0.5820 - acc: 0.8571\nEpoch 11/100\n1/1 - 0s - loss: 0.5666 - acc: 1.0000\nEpoch 12/100\n1/1 - 0s - loss: 0.5518 - acc: 1.0000\nEpoch 13/100\n1/1 - 0s - loss: 0.5375 - acc: 1.0000\nEpoch 14/100\n1/1 - 0s - loss: 0.5237 - acc: 1.0000\nEpoch 15/100\n1/1 - 0s - loss: 0.5104 - acc: 1.0000\nEpoch 16/100\n1/1 - 0s - loss: 0.4976 - acc: 1.0000\nEpoch 17/100\n1/1 - 0s - loss: 0.4852 - acc: 1.0000\nEpoch 18/100\n1/1 - 0s - loss: 0.4733 - acc: 1.0000\nEpoch 19/100\n1/1 - 0s - loss: 0.4618 - acc: 1.0000\nEpoch 20/100\n1/1 - 0s - loss: 0.4507 - acc: 1.0000\nEpoch 21/100\n1/1 - 0s - loss: 0.4400 - acc: 1.0000\nEpoch 22/100\n1/1 - 0s - loss: 0.4296 - acc: 1.0000\nEpoch 23/100\n1/1 - 0s - loss: 0.4196 - acc: 1.0000\nEpoch 24/100\n1/1 - 0s - loss: 0.4100 - acc: 1.0000\nEpoch 25/100\n1/1 - 0s - loss: 0.4006 - acc: 1.0000\nEpoch 26/100\n1/1 - 0s - loss: 0.3916 - acc: 1.0000\nEpoch 27/100\n1/1 - 0s - loss: 0.3828 - acc: 1.0000\nEpoch 28/100\n1/1 - 0s - loss: 0.3744 - acc: 1.0000\nEpoch 29/100\n1/1 - 0s - loss: 0.3662 - acc: 1.0000\nEpoch 30/100\n1/1 - 0s - loss: 0.3582 - acc: 1.0000\nEpoch 31/100\n1/1 - 0s - loss: 0.3506 - acc: 1.0000\nEpoch 32/100\n1/1 - 0s - loss: 0.3431 - acc: 1.0000\nEpoch 33/100\n1/1 - 0s - loss: 0.3359 - acc: 1.0000\nEpoch 34/100\n1/1 - 0s - loss: 0.3289 - acc: 1.0000\nEpoch 35/100\n1/1 - 0s - loss: 0.3221 - acc: 1.0000\nEpoch 36/100\n1/1 - 0s - loss: 0.3155 - acc: 1.0000\nEpoch 37/100\n1/1 - 0s - loss: 0.3091 - acc: 1.0000\nEpoch 38/100\n1/1 - 0s - loss: 0.3029 - acc: 1.0000\nEpoch 39/100\n1/1 - 0s - loss: 0.2969 - acc: 1.0000\nEpoch 40/100\n1/1 - 0s - loss: 0.2911 - acc: 1.0000\nEpoch 41/100\n1/1 - 0s - loss: 0.2854 - acc: 1.0000\nEpoch 42/100\n1/1 - 0s - loss: 0.2799 - acc: 1.0000\nEpoch 43/100\n1/1 - 0s - loss: 0.2745 - acc: 1.0000\nEpoch 44/100\n1/1 - 0s - loss: 0.2693 - acc: 1.0000\nEpoch 45/100\n1/1 - 0s - loss: 0.2643 - acc: 1.0000\nEpoch 46/100\n1/1 - 0s - loss: 0.2594 - acc: 1.0000\nEpoch 47/100\n1/1 - 0s - loss: 0.2546 - acc: 1.0000\nEpoch 48/100\n1/1 - 0s - loss: 0.2500 - acc: 1.0000\nEpoch 49/100\n1/1 - 0s - loss: 0.2454 - acc: 1.0000\nEpoch 50/100\n1/1 - 0s - loss: 0.2410 - acc: 1.0000\nEpoch 51/100\n1/1 - 0s - loss: 0.2368 - acc: 1.0000\nEpoch 52/100\n1/1 - 0s - loss: 0.2326 - acc: 1.0000\nEpoch 53/100\n1/1 - 0s - loss: 0.2286 - acc: 1.0000\nEpoch 54/100\n1/1 - 0s - loss: 0.2246 - acc: 1.0000\nEpoch 55/100\n1/1 - 0s - loss: 0.2208 - acc: 1.0000\nEpoch 56/100\n1/1 - 0s - loss: 0.2170 - acc: 1.0000\nEpoch 57/100\n1/1 - 0s - loss: 0.2134 - acc: 1.0000\nEpoch 58/100\n1/1 - 0s - loss: 0.2098 - acc: 1.0000\nEpoch 59/100\n1/1 - 0s - loss: 0.2064 - acc: 1.0000\nEpoch 60/100\n1/1 - 0s - loss: 0.2030 - acc: 1.0000\nEpoch 61/100\n1/1 - 0s - loss: 0.1997 - acc: 1.0000\nEpoch 62/100\n1/1 - 0s - loss: 0.1965 - acc: 1.0000\nEpoch 63/100\n1/1 - 0s - loss: 0.1933 - acc: 1.0000\nEpoch 64/100\n1/1 - 0s - loss: 0.1903 - acc: 1.0000\nEpoch 65/100\n1/1 - 0s - loss: 0.1873 - acc: 1.0000\nEpoch 66/100\n1/1 - 0s - loss: 0.1844 - acc: 1.0000\nEpoch 67/100\n1/1 - 0s - loss: 0.1815 - acc: 1.0000\nEpoch 68/100\n1/1 - 0s - loss: 0.1788 - acc: 1.0000\nEpoch 69/100\n1/1 - 0s - loss: 0.1761 - acc: 1.0000\nEpoch 70/100\n1/1 - 0s - loss: 0.1734 - acc: 1.0000\nEpoch 71/100\n1/1 - 0s - loss: 0.1708 - acc: 1.0000\nEpoch 72/100\n1/1 - 0s - loss: 0.1683 - acc: 1.0000\nEpoch 73/100\n1/1 - 0s - loss: 0.1658 - acc: 1.0000\nEpoch 74/100\n1/1 - 0s - loss: 0.1634 - acc: 1.0000\nEpoch 75/100\n1/1 - 0s - loss: 0.1611 - acc: 1.0000\nEpoch 76/100\n1/1 - 0s - loss: 0.1588 - acc: 1.0000\nEpoch 77/100\n1/1 - 0s - loss: 0.1565 - acc: 1.0000\nEpoch 78/100\n1/1 - 0s - loss: 0.1543 - acc: 1.0000\nEpoch 79/100\n1/1 - 0s - loss: 0.1522 - acc: 1.0000\nEpoch 80/100\n1/1 - 0s - loss: 0.1500 - acc: 1.0000\nEpoch 81/100\n1/1 - 0s - loss: 0.1480 - acc: 1.0000\nEpoch 82/100\n1/1 - 0s - loss: 0.1460 - acc: 1.0000\nEpoch 83/100\n1/1 - 0s - loss: 0.1440 - acc: 1.0000\nEpoch 84/100\n1/1 - 0s - loss: 0.1421 - acc: 1.0000\nEpoch 85/100\n1/1 - 0s - loss: 0.1402 - acc: 1.0000\nEpoch 86/100\n1/1 - 0s - loss: 0.1383 - acc: 1.0000\nEpoch 87/100\n1/1 - 0s - loss: 0.1365 - acc: 1.0000\nEpoch 88/100\n1/1 - 0s - loss: 0.1347 - acc: 1.0000\nEpoch 89/100\n1/1 - 0s - loss: 0.1330 - acc: 1.0000\nEpoch 90/100\n1/1 - 0s - loss: 0.1313 - acc: 1.0000\nEpoch 91/100\n1/1 - 0s - loss: 0.1296 - acc: 1.0000\nEpoch 92/100\n1/1 - 0s - loss: 0.1279 - acc: 1.0000\nEpoch 93/100\n1/1 - 0s - loss: 0.1263 - acc: 1.0000\nEpoch 94/100\n1/1 - 0s - loss: 0.1248 - acc: 1.0000\nEpoch 95/100\n1/1 - 0s - loss: 0.1232 - acc: 1.0000\nEpoch 96/100\n1/1 - 0s - loss: 0.1217 - acc: 1.0000\nEpoch 97/100\n1/1 - 0s - loss: 0.1202 - acc: 1.0000\nEpoch 98/100\n1/1 - 0s - loss: 0.1188 - acc: 1.0000\nEpoch 99/100\n1/1 - 0s - loss: 0.1173 - acc: 1.0000\nEpoch 100/100\n1/1 - 0s - loss: 0.1159 - acc: 1.0000\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "&lt;tensorflow.python.keras.callbacks.History at 0x279c14954c8&gt;"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 300, weights = [embedding_matrix], input_length=max_len, trainable = False)\n",
    "\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}